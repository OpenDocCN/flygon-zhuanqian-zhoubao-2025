<h1>无需等待：9 个免费方案，让你丝滑体验 DeepSeek R1</h1>
<blockquote>来源：<a href="https://zzi7a49xoa.feishu.cn/docx/RQFNdiFGfoPH8nxlzfacuaPIn2b">https://zzi7a49xoa.feishu.cn/docx/RQFNdiFGfoPH8nxlzfacuaPIn2b</a></blockquote>
<p>"DeepSeek 又崩了！"</p>

<p>这句话最近在各个 AI 交流群里经常出现。</p>

<p>眼看着别人已经用 DeepSeek 创造价值，自己却只能面对服务器繁忙的提示。</p>

<p><img src="img/800f40f51ed1b2af2fc7315010388641.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/EM0ObPcytoeWAYxw63XcbyCwnmd/"/></p>

<p>别担心！</p>

<p>经过这几天的研究，我整理出了几个稳定又免费的 DeepSeek R1 平台，让你告别排队焦虑。</p>

<p>而且不只是对话，某些平台还支持 API 免费调用，让你能真正把 AI 能力整合进工作流程。</p>

<p>最后, 我会介绍 本地部署 DeepSeek 搭建个人 AI 知识库 的详细步骤。</p>

<p>由于各大平台流量剧增，可能会出现不稳定、临时挂掉的情况。</p>
<p>如果出现挂掉可以继续选择其他平台。</p>

<h2>一、硅基流动</h2>

<p>前段时间，华为云和硅基流动联合推出了 满血版 DeepSeek R1。</p>

<p>最重要的是，注册就送 2000 万 Token。</p>

<p><img src="img/90223055c4f569a2adc4b1151557e408.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/EG84bhIgkoKhQvx0GP9cIrv0nrg/"/></p>

<p>Token 过去我的很多文章都有解释过：它可以是一个单词、标点符号、一个汉字，它的存在是把文本切分成更小的单元，方便模型处理。</p>

<p>那 2000 万 Token 是什么概念？</p>

<p>虽然不同的模型有不同的计算方式，我们可以这样简单理解：</p>

<p>假设一本普通的小说，每页有 300 个汉字（中文）或者 300 个单词（英文），那么 2000万 Token 相当于大约6.6 万页的书。</p>

<p>如果按照一本 300 页的书来算，那就是 220 多本书。相当于一个超大书架的藏书量。</p>

<p>那，在硅基流动上，具体怎么使用 DeepSeek R1 呢？</p>

<p>1、注册，地址：https://cloud.siliconflow.cn/i/TAAOvaXg</p>
<p>2、模型广场找到 R1。</p>
<p>3、输入提示词开始对话。</p>

<p><img src="img/d4ef63a3f37add71040c147532db28f5.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/K15jbOLuBohetPxwApvc7TBtnPf/"/></p>

<p>如果需要使用 API，选择模型时，我们能找到 API 文档，按照指示接入 R1 模型。</p>

<p>这里有个小技巧，利用 Cursor 或 Trae 等AI IDEA，获取 API 代码示例，就能快速完成接入。</p>

<p><img src="img/f1953f762c8a18310777c412fd8d5309.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/PmOdbD2M2otcVWxYAjVcBwRbnJc/"/></p>

<h2>二、秘塔</h2>

<p>无论是硅基流动，还是 Lambda，以及接下来介绍了各种平台，他们都会面临一个问题：联网搜索 ~</p>

<p>目前只有官方支持联网搜索，我试了下 Lambda 和硅基流动，数据都是去年的。</p>

<p><img src="img/de937ecf6aa5b3d855a30cc6c66314fc.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/EgUZbLNpaoCV37xy2ovcBXEvnad/"/></p>

<p>打开秘塔，勾上长思考试试。</p>
<p>地址：https://metaso.cn/</p>

<p><img src="img/1e5641160a8250ff4dc5197b06107609.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/JAawb0qlDoQ5hyxgOXucacAlnRh/"/></p>

<p>没错，答案就是最新的——今年七大姑八大姨饭桌上，强烈安利给我的电影——《哪吒》！</p>

<p>最近几天影评不错，听说剧情很完美，就是有点费腰，毕竟得坐 2 个半小时……😆</p>

<p><img src="img/73e76007e22fe281b832423a8db39e1c.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/XhhIbNvAHogJ32xO4qscTcXynjb/"/></p>

<h2>三、国家超算互联网</h2>

<p>国家队也开始下场，支持 DeepSeek 全民使用。地址：https://chat.scnet.cn/</p>

<p>网上很多人说有32B、14B、7B，但是我验证发现只有 DeepSeek-R1-Distill-Qwen-7B 。</p>

<p>参数是小了点，如果其他平台用不了，本地部署也没有硬件，最后再考虑用用国家超算互联网。</p>

<p><img src="img/17c5e8a07f8b64184f422cd9be9a4f29.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/BOmVbzCJ5oOMULxxK8IcKGOsnpf/"/></p>

<h2>四、Lambda.chat</h2>

<p>如果你有魔法，且只需通过聊天窗口使用，那首推 Lambda.chat，它速度超快。</p>

<p>这样能避免我们受各种天文参数、API 代码接入方式的干扰。</p>

<p>就是一个词：纯享 ~</p>
<p>地址：https://lambda.chat/</p>

<p><img src="img/3acf8edc041e71aecf17811435d3228f.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/PZtvbFzcEoQZ7ExmssHcVjKVnSc/"/></p>

<p>除了满血 671B DeepSeek R1，还支持大量不同参数的 Llama 模型选择使用。</p>

<p><img src="img/646e6ae455af225728f7dff19224e08a.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/U0ctbDr6Wott6TxY4L1c7bIZnMe/"/></p>

<h2>五、英伟达</h2>

<p>虽然 DeepSeek-R1 的出现，让 英伟达市值蒸发近 6000 亿刀。</p>
<p>但这并不妨碍英伟达 积极拥抱它，毕竟——打不过，就加入！ 😆</p>

<p>前段时间，NVIDIA 用最新的 HGX H200 服务器部署了满血版 DeepSeek-R1，直接把推理速度提升到 惊人的 3,872 Token/s！ 🚀</p>

<p>要知道 GPT-4 只有 150 Token/s，Claude-3 也不过 200-300 Token/s，英伟达简直“豪”无人性 ~</p>

<p>具体怎么用？</p>

<p>一个邮箱注册就能搞定。我测试了下，甚至免登录也可以直接使用。</p>
<p>地址： https://build.nvidia.com/deepseek-ai/deepseek-r1</p>

<p>右侧直接能看到 API 接入代码。</p>

<p><img src="img/9d128535796981780e5902afd64dc2b9.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/O1vdb6T2DojB1Ex9ULicObmVnYc/"/></p>

<h2>六、Poe</h2>

<p>作为众多国内 AI 深度用户 的日常工具，Poe 自然不会错过 DeepSeek-R1 这波巨大的流量红利。</p>

<p>官方版本 直接拉满：输入上下文 164K、输出上限 33K。</p>

<p><img src="img/6221b31f0dea3d016d89bec2b5152095.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/SWudbdiI5o8uPsxFx1xcFXftnMQ/"/></p>

<p><img src="img/f4e6e4c1798aa58d287c46649b94a005.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/HpZYb0dLGoiUaYxavgZcfQj9nMh/"/></p>

<p>如果你需要在同一个问题上生成大量文本（比如写长文档或长代码），那就可以试试 DeepSeek-R1-FW。</p>

<p>因为它支持比官方 R1 更多的输出长度，除了同时可以处理 164k 输入 token，同时还具备 164k 输出 token 的能力。</p>

<p>但是，常规问题，用 DeepSeek-R1 即可。</p>

<p><img src="img/acb2598d803e5a280b41e269a0990e36.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/YkTDbuIyuoxt5Dxb0TUcCtAbnXc/"/></p>

<h2>七、Groq</h2>

<p>除了满血的DeepSeek R1，若无需硬件条件，想测试一些大参数的蒸馏版本，比如 70B，Grop 也是一个选择。</p>

<p>登录后，在左上角找到 Deepseek-R1-Distill-Llama-70b，就能愉快玩耍啦。</p>

<p><img src="img/fbb508004caa5f06776668acaee0aec5.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/T60IbPt7JoK9xVx6oQzc0nFRnzc/"/></p>

<h2>八、阿里云百炼平台</h2>

<p>一个需要魔法的平台。</p>

<p>有一些的免费额度，可以作为备用平台。</p>

<p>地址：https://api.together.ai/playground/chat/deepseek-ai/DeepSeek-R1</p>

<p><img src="img/9ef4091d712d51731b250a9c3f476bb3.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/VaBAbjiF7oWdpxxegSoczdzwn4e/"/></p>

<h2>九、潞晨</h2>

<p>这是一个完全适合开发者的 DeepSeek API接入平台。</p>

<p>普通用户，如果不关心 API，可以跳过这一章，继续往下看。</p>

<p>对于开发者来说 ，潞晨云平台为 DeepSeek-R1 系列模型，提供从满血 671B 大模型到高效蒸馏小模型多种灵活选择。</p>

<p>重点来了，目前 API 无限量限时免费开放体验。</p>
<p>地址：https://cloud.luchentech.com/maas/modelMarket</p>

<p>至于能用多久，暂时未知。</p>

<p><img src="img/632020f495de81f11e1c14e9a07df61b.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/QBsmbFG9DoQ3QvxqY2ucAYNTnVb/"/></p>

<p>使用方法：</p>

<p>1、生成密钥。</p>

<p><img src="img/e9040cdcd266089c99c637d37b1f37b1.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/GFDHbJJzFo1JDtx0v3scDIOgnqy/"/></p>

<p>2、打开 API 文档，找到代码样例。比如，这里我找到了 curl example。</p>

<p><img src="img/7fa6ca5cd2a2c324ac287428e572257f.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/KYueb6gkHomIVfxpDITcRm5Zn9e/"/></p>

<p>导入 curl 代码进入在线 ApiFox，测试连通性，地址：https://app.apifox.com</p>
<p><img src="img/7bf0028d9821dc94e9c7863db3d41902.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/TIoUbS4puogoNFxIr7MchAkSnqe/"/></p>

<p>3、粘贴从官方复制过来的密钥。</p>

<p><img src="img/e5ccf980f99746449e28b224fecbb33f.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/RFHzbqxfxo6G60xRrLrcFnOjnFf/"/></p>

<p>4、点击发送进行测试。搞定~</p>

<p><img src="img/4d146fbf007a0b6e258a92048bb7fb2c.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/WY5bbdXqloSavpx2IEGcm48GnBh/"/></p>

<p>接下来具体怎么用，有技术背景的朋友想必不需要我多介绍啦~</p>

<p>要是没有技术背景，可以参考我过去写的 AI 编程文章，借助 Cursor 辅助完成应用开发。</p>

<h2>十、本地部署</h2>

<p>最近很多朋友都在问：怎么本地部署 DeepSeek 搭建个人知识库。</p>
<p><img src="img/24adba709c80bdd4d03eccbca718c485.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/J4debLhVYomRpix86b1cDsGvnzh/"/></p>

<p>老实说，如果你不是为了研究技术，或者确实需要保护涉密数据，我真不建议去折腾本地部署。</p>

<p>为什么呢？</p>

<p>目前 Ollama 从 1.5B 到 70B 都只是把 R1 的推理能力提炼到 Qwen 和 Llama 的蒸馏版本（学生模型）上。</p>

<p>虽说性能是提升了不少，但跟原汁原味的 R1 模型比起来，还是差太多了。</p>

<p>官方的满血版本可是 671B 的参数量，说实话，对普通人来说想本地部署，这成本确实太高了。</p>

<p>不过我最近发现了一个平台，不仅能用上 R1 本 1，还提供了不少免费额度。</p>

<p>此外，为了让拥有算力的朋友可以本地部署，我也提供了相关教程。</p>

<p>本章你会收获：</p>

<ol><li>满血 DeepSeek R1 模型 API 搭建个人知识库</li></ol>
<ol><li>本地部署 DeepSeek R1 模型 搭建个人知识库</li></ol>

<h3>一）个人知识库使用效果（满血版）</h3>

<p>来看几个使用案例：如何借助 个人知识库文件 作为外脑，用方法论指导我们正确做事？</p>

<p>DeepSeek 确实很好用，但关键还是会不会提问。</p>
<p>如果不会提问，AI 再强也帮不上忙。</p>

<p>除了花时间学习提示词，更快的方式是本地备一份提问指南，让 AI 指导你该怎么正确提问，这样才能真正发挥它的价值！</p>

<p><img src="img/b9eca90a925b6d70ba3576b02f4a65ff.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/WK0PbXK6JobJuYxl96bcBIM1nWg/"/></p>

<p>AI 借助知识库内的 DeepSeek 指导手册，预判了我可能想问的问题，以及建议的正确提示词格式。</p>

<p>从回答中可以发现，AI 不是依据自身语料库回复，而是基于知识库内容回复。</p>

<p><img src="img/2a350c47f23f3101487b05342d1b7b99.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/FCuBbLI5koQJQXx6hgscOWS5n7C/"/></p>

<p>当然，我们也可以直接搜索知识库的原始信息，从而快速查询信息。</p>

<p><img src="img/d5d3f21501452d1e96a7100608a026a0.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/R7EhbZRxGoJic4xFaPkcnc1Cnwj/"/></p>

<h3>二）API  搭建知识库</h3>

<p>如果本地数据不涉密，还想获得最佳使用效果，那肯定得选满血的 DeepSeek R1 模型。</p>

<p>我们来看看怎么利用 API 用上满血的 R1 模型（671 B）。</p>

<p>1、先下载一个叫 Cherry Studio 的软件。</p>
<p>地址： https://cherry-ai.com/download</p>

<p><img src="img/61ae7de226082183cc26efde5f323052.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/UK4QbWk0HooiHnxs8RVc1oWEn5f/"/></p>

<p>2、登录/注册「硅基流动」，新用户会赠送 2000万 Token 额度。</p>
<p>地址：https://cloud.siliconflow.cn/i/TAAOvaXg</p>

<p><img src="img/465f590a0eb7242f498ad03e97752b84.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/HYikbzUstoF0TIxqmWZcwdOmnqd/"/></p>

<p>3、来到 API 密钥生成界面，创建或者复制已有的密钥。</p>

<p><img src="img/afa763773a74bcb329e134b607531ee7.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/O0frbF57VohN1OxUrXMcL1Sfnwm/"/></p>

<p>4、来到 Cherry Studio，配置 API Key。</p>

<p><img src="img/ba90130faec724e992d8984036a0604b.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/L9LhbzY6CoHuNmxQ5vIc6QhynFc/"/></p>

<p>5、在模型广场首页，排在前两位的就是「硅基流动」和「华为云」合作发布的 DeepSeek R1 / V3 模型。</p>

<p>如果需要推理能力，记得打开并复制 R1 模型的名称。</p>

<p><img src="img/843ed86675f7a7390f989d2ce411a3f8.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/WU54bRFSWosQ8kxEt8qc9z5rnbc/"/></p>

<p>6、在模型服务的硅基流动下方，添加 R1 模型。</p>

<p><img src="img/042dbe556931d31a6504ac4bccf68f84.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/Taexbs9JkofqW5xwFZScgmMfnMc/"/></p>

<p>7、记得点击检查，测试下 API 是否可以正常访问。</p>

<p><img src="img/799b3e74b7c14f8702cd89fd18a50ef4.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/D5wNbTr8CoLFwXxvdQCcS8Yinnh/"/></p>

<p>8、现在对话模型有了 R1，还缺少一个嵌入模型。</p>

<p>嵌入模型的主要作用是将本地文件的内容转换成有意义的数字，存储到向量数据库中。</p>

<p>在用户提问时，利用 RAG 技术在数据库中搜索到相似答案，最终回复用户。</p>

<p>过去我有通俗解释过 RAG 技术，大家如果不了解，可以回头看下：</p>

<p>我们再配置一个向量模型：BAAI/bge-m3，如果希望搜索的精准度更高，可以选择 Pro/BAAI/bge-m3。</p>

<p><img src="img/fc7467c24ceadb947a27c870a7d532e5.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/PKkHbiilIokoroxWE2RcxDMknPc/"/></p>

<p>按照同样的方式配置到 Cherry Studio 中，这里不需要点击检查。</p>

<p><img src="img/9198cf091f6ed94bfca25dd5cecfda58.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/DjzMbYUK7ooEmhxwqJ7cZoupnnh/"/></p>

<p>9、在 Cherry Studio 创建知识库，选择刚才配置的嵌入模型，这样就会自动利用对应的模型来向量化数据。</p>

<p><img src="img/1bef25494b85f20b411a280647db04c2.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/HT10bNCR8oKZGwx5pgac2CBunfg/"/></p>

<p>10、上传本地文件进行向量化。</p>

<p>如果本地 PDF 文件是 扫描件、手写件，或者带有复杂的表格 和 数学公式，解析效果会很差，甚至无法解析。</p>

<p>遇到这种情况，建议配合过去我介绍的 PDF 转结构化文档 的方案来使用！！！</p>

<p>这样才可以正常回复知识库的内容：</p>

<p>如果追求执行比，推荐使用 Doc2x：https://doc2x.noedgeai.com?inviteCode=4A6KOD</p>

<p>如果希望更加稳定，那么可以考虑 Textin ：https://www.textin.com/market/detail/pdf_to_markdown</p>

<p>当我们上传文件后，箭头指向的图标如图所示，则代表向量化成功。</p>

<p><img src="img/46597aa07e07e4336791dc1f65634e99.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/MfFRbROkborbo7xWArpcjdkfnPe/"/></p>

<p>11、测试使用，这一步添加助手，并选择刚配置的 满血 R1 模型。</p>

<p><img src="img/71bbaf58b7811e2ef13e61e41d5419c2.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/Iv8kbuHFXoWdiOxQNGKc7wm2ntc/"/></p>

<p>如果不想每次在添加助手时选择模型，可以将它设置为 默认模型。</p>

<p><img src="img/a9540ede74faa3df3520f0a636104f14.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/MmumbJfMqoHcf0xlmGMcYQJXnIb/"/></p>

<p>我们来测试一下，发现 DeepSeek 已经开始深度思考了。</p>

<p><img src="img/7c5fd82ad638ba06dc9858c8846374b1.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/MGG1bd8RXoYOEJxjEFEcZboonSg/"/></p>

<p>AI 回复的答案和原文一致。</p>

<p><img src="img/d1e8ba66772c826e1e270920d75282ea.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/RFCabY0QLohmP0x7d9wcb7AhnUf/"/></p>

<p>原文内容：</p>

<p><img src="img/aaf33e0d0ff7376bfcb4a28c06a7ee51.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/U9bubV3G2owUSPxSqKUckelunVf/"/></p>

<h3>三）本地部署搭建知识库</h3>

<p>如果只是想 简单体验，或者本地算力充足、希望保护数据安全，那么可以考虑 本地部署 的方案。</p>

<p>1、访问 Ollama 官方地址：https://ollama.com/download，下载软件。</p>
<p>建议下载最新版本 Ollama，个人踩坑经历：旧版本安装 R1 后可能无法正常使用。</p>

<p><img src="img/e5e4ab4c647d5270458ce843d4b27e1a.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/MfKfbKHOdoaD5Ox6h3IcobcPnIc/"/></p>

<p>2、双击运行 Ollama 后，打开命令行，运行需要安装的模型（参数越大，显存要求越高）。</p>
<p>地址：https://ollama.com/library/deepseek-r1</p>

<p><img src="img/322eb9051eaa96cb80fdfba58e3e7101.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/K9uAbbm9DozcoUx7gM3cmpdcnPd/"/></p>

<p>如果没有 GPU，建议选择 1.5B，运行相对轻量。我这 4G 显存 勉强能跑 8B，但速度较慢。</p>
<p>有朋友用 RTX 4090 测试 32B，效果和速度都不错。大家可以参考这个梯度，根据自己的硬件选择合适的模型。。</p>

<p>下载并运行成功后，就可以和 DeepSeek R1 对话啦。</p>

<p><img src="img/a6c59514f826e44b907b4f769f423b77.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/SPAKbG9utoXTZYxGNdwcrJiwnqe/"/></p>

<p>当然，我们可以用 Cherry Studio 作为本地 R1 模型的 UI 界面。</p>

<p>打开软件的配置，选择 Ollama，在管理按钮中选择自己部署的模型。</p>
<p>如果选不到，就手动添加模型，API 秘钥填不填无所谓）</p>

<p><img src="img/343be680abb309aea8c2e8b2bdcc2b09.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/UiAxbSzaIoukFNxCDdQceVe9ndd/"/></p>

<p>最后点击检查，测试下网络连通性，出现连接成功即可。</p>

<p><img src="img/4b25a4391e5d2e6c5051defb9ee04314.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/FAH2bDWacoQrxVx4IiFcUNaEnQf/"/></p>

<p>接下来就可以在添加助手时，选择本地部署的 R1 模型啦。</p>

<p><img src="img/4e1dbf3247a17e9954745cc8aa63cec9.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/KJqSbt9GCoHeZ6xeQ6Mcb0oRnth/"/></p>

<p>再测试使用一下，答案与原文一致，搞定~</p>

<p><img src="img/2513f3f17d728b16f4f7ccd4c93015d0.png" data-original-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/v2/cover/MAa2bFo62oEpx2xkV7DcPGNungg/"/></p>

<p>因为我用的是 Mac，所以没测试 Windows 设备。不过两者区别不大，大家可以自行尝试下。</p>

<p>看到这里，相信你已经找到最适合自己的 DeepSeek 个人知识库方案了。</p>

<p>如果你和大多数人一样，选择使用官方 API ，那就不用再为性能和本地硬件资源发愁；</p>

<p>如果你恰好有充足的算力和技术储备，本地部署也是个不错的选择。</p>

<h2>十一、总结</h2>

<p>本次介绍的这些平台，每个都各具特色。</p>

<p>对国内用户来说，秘塔、硅基流动 是不错的选择。</p>

<p>如果在国外，可以选择国外平台： Lambda、Poe、英伟达。</p>
<p>如果有足够优质的硬件资源，毫无疑问，可以选择本地部署。毕竟数据在本地，才最安全。</p>

<p>每一次技术的迭代，都会带来更多可能。当一个平台暂时无法访问，自然会有新的方案可以接力。</p>

<p>这就是科技发展的魅力 —— 永远充满希望。</p>

<p>因为，我们总能找到更好的解决方案，不是吗？</p>

<p>最后，在整个过程中，我想和你分享的不仅仅是这些技术细节。</p>

<p>而是一个中国团队能做出世界级大模型，一个需要 +86 才能注册的大模型。</p>

<p>很庆幸，我们能作为中国 AI 领域技术进步的见证人。</p>

<p>当然，暂时的领先不是终点。</p>

<p>但 DeepSeek 给我们带来的，是“中国也可以”的这份信心。</p>

<p>这，才是我们真正要守护的希望。</p>

<p>最后，感谢各大平台的无私奉献 ，祝大家玩的开心 ~</p>

<p>我是 🐼 熊猫 Jay，下次再见～</p>
